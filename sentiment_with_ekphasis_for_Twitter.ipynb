{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kennethfargose/Sentiment-for-Twitter/blob/main/sentiment_with_ekphasis_for_Twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aalxbFeptbgl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCu9_Krvtdmx",
        "outputId": "8023e907-a326-4fae-f93f-a5d39d03519b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = pd.read_csv('/content/drive/MyDrive/tweets.csv')\n",
        "tweets.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "tweets.set_index('id')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "z7dKLLpmtkm3",
        "outputId": "badb2d3c-861b-431a-e1ca-65857b3fdf35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a907ec7e-ccc6-4f49-91c4-7c4ad862492b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>263791921753882624</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Some areas of New England could see the first ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264194578381410304</th>\n",
              "      <td>negative</td>\n",
              "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264041328420204544</th>\n",
              "      <td>neutral</td>\n",
              "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263816256640126976</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263722278712393728</th>\n",
              "      <td>positive</td>\n",
              "      <td>Tim Tebow may be availible ! Wow Jerry \\u002c ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639855845958885376</th>\n",
              "      <td>positive</td>\n",
              "      <td>@Racalto_SK ok good to know. Punting at MetLif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639979760735662080</th>\n",
              "      <td>neutral</td>\n",
              "      <td>everyone who sat around me at metlife was so a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>640196838260363269</th>\n",
              "      <td>neutral</td>\n",
              "      <td>what giants or niners fans would wanna go to t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>640975710354567168</th>\n",
              "      <td>positive</td>\n",
              "      <td>Anybody want a ticket for tomorrow Colombia vs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>641034340068143104</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Mendez told me he'd drive me to MetLife on Sun...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50121 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a907ec7e-ccc6-4f49-91c4-7c4ad862492b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a907ec7e-ccc6-4f49-91c4-7c4ad862492b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a907ec7e-ccc6-4f49-91c4-7c4ad862492b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                   Sentiment                                              Tweet\n",
              "id                                                                             \n",
              "263791921753882624   neutral  Some areas of New England could see the first ...\n",
              "264194578381410304  negative  @francesco_con40 2nd worst QB. DEFINITELY Tony...\n",
              "264041328420204544   neutral  #Thailand Washington - US President Barack Oba...\n",
              "263816256640126976   neutral  Did y\\u2019all hear what Tony Romo dressed up ...\n",
              "263722278712393728  positive  Tim Tebow may be availible ! Wow Jerry \\u002c ...\n",
              "...                      ...                                                ...\n",
              "639855845958885376  positive  @Racalto_SK ok good to know. Punting at MetLif...\n",
              "639979760735662080   neutral  everyone who sat around me at metlife was so a...\n",
              "640196838260363269   neutral  what giants or niners fans would wanna go to t...\n",
              "640975710354567168  positive  Anybody want a ticket for tomorrow Colombia vs...\n",
              "641034340068143104   neutral  Mendez told me he'd drive me to MetLife on Sun...\n",
              "\n",
              "[50121 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ekphrasis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiI-QBDXuWbm",
        "outputId": "d685b94b-5a76-4580-8d07-95ad38dc4819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.62.3)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (0.4.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.19.5)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (5.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Baseline\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tSySw5SuLBT",
        "outputId": "11f5fabf-c1a7-4032-f97b-02a7e707e21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeXtzqZVDbDd",
        "outputId": "a2d8bbc3-336e-4558-bace-105d7d430767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7ZuOH_bEVQA",
        "outputId": "470d91ca-eab4-4b85-e812-779ff32406f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.58      0.35      0.44      1578\n",
            "     neutral       0.63      0.64      0.63      4510\n",
            "    positive       0.62      0.71      0.66      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.57      0.58     10025\n",
            "weighted avg       0.62      0.62      0.61     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENTS HERE"
      ],
      "metadata": {
        "id": "rLpC621CGE7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MNB baseline\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "    #     'time', 'url', 'date', 'number'],\n",
        "    # annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "    #     'emphasis', 'censored'},\n",
        "    # segmenter=\"twitter\", \n",
        "    # corrector=\"twitter\", \n",
        "    # dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "\n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV0hLNKnvi49",
        "outputId": "3e5c3319-7aa3-455e-fd2b-224b2ae2cd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading english - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/english/counts_1grams.txt\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.65      0.24      0.35      1578\n",
            "     neutral       0.62      0.68      0.65      4510\n",
            "    positive       0.62      0.71      0.66      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.63      0.54      0.55     10025\n",
            "weighted avg       0.63      0.62      0.61     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add normalization\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number']\n",
        "    # annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "    #     'emphasis', 'censored'},\n",
        "    # segmenter=\"twitter\", \n",
        "    # corrector=\"twitter\", \n",
        "    # dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "\n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xORvnNIWJ3sF",
        "outputId": "9619cfd1-0685-4b7f-ff8b-c4ad89581c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading english - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.59      0.35      0.44      1578\n",
            "     neutral       0.63      0.64      0.64      4510\n",
            "    positive       0.62      0.71      0.66      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.62      0.57      0.58     10025\n",
            "weighted avg       0.62      0.62      0.62     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add annotationcreate temporary table tankedLiters as (SELEct uua.*, SUM(volume) as totaal from userUsesApps uua join transaction t on uua.personid = t.account_id join fuel_transaction ft on t.id = ft.id where t.type = 'fuelvisit' \n",
        "and t.date > '2022-02-01' group by uua.personid);\n",
        "\n",
        "create temporary table parkTransactions as (SELEct tl.*, COUNT(pr.id) as totaal from tankedLiters tl join transaction t on tl.personid = t.account_id join parking_record pr on t.id = pr.id where t.type = 'parkingrecord' \n",
        "and t.date > '2022-02-01' group by tl.personid);\n",
        "\n",
        "select * from parkTransactions\n",
        "\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "    #     'time', 'url', 'date', 'number']\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'}\n",
        "    # segmenter=\"twitter\", \n",
        "    # corrector=\"twitter\", \n",
        "    # dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "\n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPOe-HCBKH6k",
        "outputId": "6179b83d-e0dd-4984-9153-5f7e637725c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading english - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.65      0.24      0.35      1578\n",
            "     neutral       0.62      0.67      0.64      4510\n",
            "    positive       0.62      0.71      0.66      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.63      0.54      0.55     10025\n",
            "weighted avg       0.62      0.62      0.60     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add segmentation\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "    #     'time', 'url', 'date', 'number']\n",
        "    # annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "    #     'emphasis', 'censored'}\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    # corrector=\"twitter\", \n",
        "    # dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "\n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-yiEqEYKrat",
        "outputId": "0969cb35-01de-49d9-e49e-9ba44eaac711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading english - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.65      0.26      0.38      1578\n",
            "     neutral       0.62      0.66      0.64      4510\n",
            "    positive       0.62      0.71      0.66      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.63      0.55      0.56     10025\n",
            "weighted avg       0.62      0.62      0.61     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add spell correction\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "    #     'time', 'url', 'date', 'number']\n",
        "    # annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "    #     'emphasis', 'censored'}\n",
        "    # segmenter=\"twitter\", \n",
        "    # unpack_hashtags=True,\n",
        "    # unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    spell_correct_elong=True\n",
        "    # dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "\n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sGe5V0ZLMDE",
        "outputId": "b27b40e8-6b05-4e6c-8778-041dfeb727a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.65      0.24      0.35      1578\n",
            "     neutral       0.62      0.68      0.65      4510\n",
            "    positive       0.62      0.71      0.66      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.63      0.54      0.55     10025\n",
            "weighted avg       0.63      0.62      0.61     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add dictionary for emoticons\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "    #     'time', 'url', 'date', 'number']\n",
        "    # annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "    #     'emphasis', 'censored'}\n",
        "    # segmenter=\"twitter\", \n",
        "    # unpack_hashtags=True,\n",
        "    # unpack_contractions=True,\n",
        "    # corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "\n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pTwB7cbQK3S",
        "outputId": "26917e0c-1a1b-48c6-8edf-8afd48e0ce76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading english - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.65      0.24      0.35      1578\n",
            "     neutral       0.62      0.68      0.65      4510\n",
            "    positive       0.62      0.71      0.66      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.63      0.54      0.55     10025\n",
            "weighted avg       0.63      0.62      0.61     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# together\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    # segmenter=\"twitter\", \n",
        "    # unpack_hashtags=True,\n",
        "    # unpack_contractions=True,\n",
        "    # corrector=\"twitter\", \n",
        "    # dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "\n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f12QyPkmmSR",
        "outputId": "85b6771f-4313-4d2e-b4f9-c1ebb83575d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading english - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.58      0.35      0.44      1578\n",
            "     neutral       0.63      0.64      0.63      4510\n",
            "    positive       0.62      0.71      0.66      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.57      0.58     10025\n",
            "weighted avg       0.62      0.62      0.62     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# together\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    # corrector=\"twitter\", \n",
        "    # dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "\n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4luCUonom0K4",
        "outputId": "b58b843d-ee78-45a7-bb88-effa25421206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading english - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.57      0.38      0.46      1578\n",
            "     neutral       0.64      0.62      0.63      4510\n",
            "    positive       0.62      0.72      0.67      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.58      0.59     10025\n",
            "weighted avg       0.62      0.62      0.62     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# together\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    # segmenter=\"twitter\", \n",
        "    # unpack_hashtags=True,\n",
        "    # unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "\n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47Er-OWdm_4p",
        "outputId": "896c6050-1248-469a-dcc6-9c36e15f68fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.58      0.35      0.44      1578\n",
            "     neutral       0.63      0.64      0.63      4510\n",
            "    positive       0.62      0.71      0.66      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.57      0.58     10025\n",
            "weighted avg       0.62      0.62      0.61     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Everything together\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "\n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBYTQvtnRCzU",
        "outputId": "239fea26-e072-4a75-b9ec-bfc7183afde0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.57      0.38      0.46      1578\n",
            "     neutral       0.64      0.62      0.63      4510\n",
            "    positive       0.62      0.72      0.67      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.58      0.59     10025\n",
            "weighted avg       0.62      0.62      0.62     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OWN ADDITIONS"
      ],
      "metadata": {
        "id": "vaIYOMWxRrc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add lowercasing\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s).lower() for s in sentences]\n",
        "\n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iBWb9wXJfIS",
        "outputId": "3fe25d25-af55-496c-b097-631db2cd608d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.57      0.38      0.46      1578\n",
            "     neutral       0.64      0.62      0.63      4510\n",
            "    positive       0.62      0.72      0.67      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.58      0.59     10025\n",
            "weighted avg       0.62      0.62      0.62     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s).lower() for s in sentences]\n",
        "no_stop_words = []\n",
        "for sentence in preprocessed:\n",
        "    no_stop_words.append(' '.join([word for word in sentence.split(' ') if word.lower() not in stopwords.words('english')]))\n",
        "    \n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(no_stop_words, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHqVqMNASPvy",
        "outputId": "02bfae40-3167-4e74-91f5-d639b8aa140b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.59      0.37      0.45      1578\n",
            "     neutral       0.63      0.62      0.62      4510\n",
            "    positive       0.62      0.71      0.66      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.57      0.58     10025\n",
            "weighted avg       0.62      0.62      0.61     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "print(' '.join([lemmatizer.lemmatize(word) for word in sentences[0].split(' ')]))\n",
        "lemmatized = []\n",
        "for sentence in preprocessed:\n",
        "    lemmatized.append(' '.join([lemmatizer.lemmatize(word) for word in sentence.split(' ')]))\n",
        "    \n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(lemmatized, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp19UZ2-SyY_",
        "outputId": "5849c144-4d5d-4d68-ec62-dd573fa172a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n",
            "Some area of New England could see the first flake of the season Tuesday.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.57      0.39      0.46      1578\n",
            "     neutral       0.63      0.62      0.63      4510\n",
            "    positive       0.62      0.72      0.67      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.58      0.58     10025\n",
            "weighted avg       0.62      0.62      0.62     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "print(' '.join([ps.stem(word) for word in sentences[0].split(' ')]))\n",
        "stemmed = []\n",
        "for sentence in preprocessed:\n",
        "    stemmed.append(' '.join([ps.stem(word) for word in sentence.split(' ')]))\n",
        "    \n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(stemmed, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_0RUtrkVtbZ",
        "outputId": "8b8e2674-d1b5-47cd-d204-820e43598b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n",
            "some area of new england could see the first flake of the season tuesday.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.56      0.40      0.47      1578\n",
            "     neutral       0.64      0.62      0.63      4510\n",
            "    positive       0.62      0.72      0.67      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.58      0.59     10025\n",
            "weighted avg       0.62      0.62      0.62     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# No punctuation\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import PorterStemmer\n",
        "import string\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ps = PorterStemmer()\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "print(' '.join([word.translate(str.maketrans('', '', string.punctuation)) for word in sentences[0].split(' ')]))\n",
        "no_punct = []\n",
        "for sentence in preprocessed:\n",
        "    no_punct.append(' '.join([word.translate(str.maketrans('', '', string.punctuation)) for word in sentence.split(' ')]))\n",
        "    \n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(no_punct, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eRTI-3EYu4k",
        "outputId": "76a82178-87b6-454b-bd95-1367ac900354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n",
            "Some areas of New England could see the first flakes of the season Tuesday\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.58      0.35      0.43      1578\n",
            "     neutral       0.63      0.63      0.63      4510\n",
            "    positive       0.62      0.73      0.67      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.57      0.58     10025\n",
            "weighted avg       0.62      0.62      0.61     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine techniques lemmatization and no punctuation\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import PorterStemmer\n",
        "import string\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ps = PorterStemmer()\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "new_sentences = []\n",
        "for sentence in preprocessed:\n",
        "    new_sentences.append(' '.join([lemmatizer.lemmatize(word.translate(str.maketrans('', '', string.punctuation))) for word in sentence.split(' ')]))\n",
        "    \n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_sentences, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws7M24H8Zei6",
        "outputId": "b752b43e-e8d3-4708-da25-38f8c5102bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n",
            "some areas of new england could see the first flakes of the season tuesday\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.58      0.34      0.43      1578\n",
            "     neutral       0.63      0.63      0.63      4510\n",
            "    positive       0.62      0.72      0.67      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.56      0.58     10025\n",
            "weighted avg       0.62      0.62      0.61     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine techniques lemmatization and lowercase\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import PorterStemmer\n",
        "import string\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ps = PorterStemmer()\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "new_sentences = []\n",
        "for sentence in preprocessed:\n",
        "    new_sentences.append(' '.join([lemmatizer.lemmatize(word).lower() for word in sentence.split(' ')]))\n",
        "    \n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_sentences, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiCKdCugd_uH",
        "outputId": "63fb44f9-af25-4d02-bda7-05b97ab0cb73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.57      0.39      0.46      1578\n",
            "     neutral       0.63      0.62      0.63      4510\n",
            "    positive       0.62      0.72      0.67      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.58      0.58     10025\n",
            "weighted avg       0.62      0.62      0.62     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine techniques lemmatization and lowercase and no punctuation\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import PorterStemmer\n",
        "import string\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ps = PorterStemmer()\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "new_sentences = []\n",
        "for sentence in preprocessed:\n",
        "    new_sentences.append(' '.join([lemmatizer.lemmatize(word.translate(str.maketrans('', '', string.punctuation))).lower() for word in sentence.split(' ')]))\n",
        "    \n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_sentences, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-aeovNyf49k",
        "outputId": "6f5990c2-60c6-4134-911b-bbb6dea62ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.58      0.34      0.43      1578\n",
            "     neutral       0.63      0.63      0.63      4510\n",
            "    positive       0.62      0.72      0.67      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.56      0.58     10025\n",
            "weighted avg       0.62      0.62      0.61     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine techniques stemming and lowercase\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import PorterStemmer\n",
        "import string\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ps = PorterStemmer()\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "new_sentences = []\n",
        "for sentence in preprocessed:\n",
        "    new_sentences.append(' '.join([ps.stem(word).lower() for word in sentence.split(' ')]))\n",
        "    \n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_sentences, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bQxOXxzesqh",
        "outputId": "be9d3a53-9649-4222-fcbf-c8722a4d6897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.56      0.40      0.47      1578\n",
            "     neutral       0.64      0.62      0.63      4510\n",
            "    positive       0.62      0.72      0.67      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.58      0.59     10025\n",
            "weighted avg       0.62      0.62      0.62     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine techniques stemming and no punctuation\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import PorterStemmer\n",
        "import string\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ps = PorterStemmer()\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "new_sentences = []\n",
        "for sentence in preprocessed:\n",
        "    new_sentences.append(' '.join([ps.stem(word.translate(str.maketrans('', '', string.punctuation))) for word in sentence.split(' ')]))\n",
        "    \n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_sentences, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIJjM2dWfgk0",
        "outputId": "879f4d3c-2389-4d9d-ed35-c51fc3c5a244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.57      0.38      0.46      1578\n",
            "     neutral       0.64      0.62      0.63      4510\n",
            "    positive       0.62      0.72      0.67      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.58      0.59     10025\n",
            "weighted avg       0.62      0.62      0.62     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine techniques stemming and lowercase and no punctuation\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import PorterStemmer\n",
        "import string\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ps = PorterStemmer()\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "new_sentences = []\n",
        "for sentence in preprocessed:\n",
        "    new_sentences.append(' '.join([ps.stem(word.translate(str.maketrans('', '', string.punctuation))).lower() for word in sentence.split(' ')]))\n",
        "    \n",
        "\n",
        "vect = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_sentences, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "\n",
        "predicted_NB = classifier.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UCjaVd-gXck",
        "outputId": "1884da82-dfd6-4c52-a835-bcc29509b7b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.57      0.38      0.46      1578\n",
            "     neutral       0.64      0.62      0.63      4510\n",
            "    positive       0.62      0.72      0.67      3937\n",
            "\n",
            "    accuracy                           0.62     10025\n",
            "   macro avg       0.61      0.58      0.59     10025\n",
            "weighted avg       0.62      0.62      0.62     10025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENTATION WITH MULTINB SETUP"
      ],
      "metadata": {
        "id": "aYG-GeKthGd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best performing set up\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    corrector=\"twitter\", \n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = tweets['Tweet'].tolist()\n",
        "preprocessed = [text_processor.pre_process_doc(s) for s in sentences]\n",
        "new_sentences = []\n",
        "for sentence in preprocessed:\n",
        "    new_sentences.append(' '.join([ps.stem(word).lower() for word in sentence.split(' ')]))\n",
        "    \n",
        "\n",
        "vect = CountVectorizer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_sentences, tweets['Sentiment'].tolist(), test_size=0.2, random_state=42)\n",
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btZ3miwbhJ6T",
        "outputId": "86400eaf-342b-4c49-c12c-ac1ab066280d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "clf = MultinomialNB()\n",
        "parameters = {  \n",
        "    'alpha': [4, 2, 1.5, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
        "    'fit_prior': [True, False] ,\n",
        "    'class_prior': [None, [0.333, 0.333, 0.333], [0.2, 0.2, 0.6], [0.2, 0.6, 0.2], [0.2, 0.2, 0.6], [0.4, 0.2, 0.4], [0.4, 0.4, 0.2], [0.2, 0.4, 0.4], [0.35, 0.35, 0.3], [0.3, 0.35, 0.35], [0.35, 0.3, 0.35],\n",
        "                    [0.5, 0.25, 0.25], [0.25, 0.5, 0.25], [0.25, 0.25, 0.5], [0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]]\n",
        "}  \n",
        "grid_search = GridSearchCV(clf, parameters, scoring='recall_macro')\n",
        "grid_search.fit(X_train_dtm, y_train)\n",
        "best = grid_search.best_estimator_\n",
        "predicted_NB = best.predict(X_test_dtm)\n",
        "print(classification_report(y_test, predicted_NB))\n",
        "print(grid_search.best_params_)\n",
        "#print(grid_search.cv_results_)\n",
        "accuracies = pd.concat([pd.DataFrame(grid_search.cv_results_[\"params\"]),pd.DataFrame(grid_search.cv_results_[\"mean_test_score\"], columns=[\"MAR\"])],axis=1)\n",
        "print(accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzMFHTQYhmEE",
        "outputId": "8a97382c-f7fb-4bbb-aa27-54374d1e3c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.40      0.68      0.50      1578\n",
            "     neutral       0.67      0.50      0.57      4510\n",
            "    positive       0.66      0.66      0.66      3937\n",
            "\n",
            "    accuracy                           0.59     10025\n",
            "   macro avg       0.58      0.62      0.58     10025\n",
            "weighted avg       0.62      0.59      0.60     10025\n",
            "\n",
            "{'alpha': 1, 'class_prior': [0.9, 0.05, 0.05], 'fit_prior': True}\n",
            "       alpha            class_prior  fit_prior       MAR\n",
            "0    4.00000                   None       True  0.481271\n",
            "1    4.00000                   None      False  0.485467\n",
            "2    4.00000  [0.333, 0.333, 0.333]       True  0.485467\n",
            "3    4.00000  [0.333, 0.333, 0.333]      False  0.485467\n",
            "4    4.00000        [0.2, 0.2, 0.6]       True  0.469756\n",
            "..       ...                    ...        ...       ...\n",
            "301  0.00001      [0.9, 0.05, 0.05]      False  0.514175\n",
            "302  0.00001      [0.05, 0.9, 0.05]       True  0.520374\n",
            "303  0.00001      [0.05, 0.9, 0.05]      False  0.520374\n",
            "304  0.00001      [0.05, 0.05, 0.9]       True  0.508308\n",
            "305  0.00001      [0.05, 0.05, 0.9]      False  0.508308\n",
            "\n",
            "[306 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracies.sort_values(by=['MAR'], ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui0U3XDfyvQW",
        "outputId": "796abb6c-382f-4888-b7d1-53642ba348ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     alpha        class_prior  fit_prior       MAR\n",
            "130    1.0  [0.9, 0.05, 0.05]       True  0.609125\n",
            "131    1.0  [0.9, 0.05, 0.05]      False  0.609125\n",
            "153    0.1  [0.35, 0.35, 0.3]      False  0.603558\n",
            "152    0.1  [0.35, 0.35, 0.3]       True  0.603558\n",
            "157    0.1  [0.35, 0.3, 0.35]      False  0.602959\n",
            "..     ...                ...        ...       ...\n",
            "64     2.0  [0.05, 0.9, 0.05]       True  0.431819\n",
            "33     4.0  [0.05, 0.05, 0.9]      False  0.417764\n",
            "32     4.0  [0.05, 0.05, 0.9]       True  0.417764\n",
            "30     4.0  [0.05, 0.9, 0.05]       True  0.408966\n",
            "31     4.0  [0.05, 0.9, 0.05]      False  0.408966\n",
            "\n",
            "[306 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qpZa6srTkZMw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}